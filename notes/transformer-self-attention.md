# 自注意力机制解密：Transformer为何能看透序列关系的本质？
> 本文以矩阵运算为切入点，通过可视化推理和类比分析，深度剖析Transformer中自注意力机制的设计哲学。从查询-键值矩阵的数学本质，到Softmax归一化的概率意义，层层拆解序列元素间关联权重的生成逻辑。文章通过仿真实例揭示Q、K、V矩阵的协同机制，诠释"权重即关联"的核心思想，并探讨该设计对传统RNN架构的颠覆性突破。适合希望穿透公式表象、理解注意力机制底层逻辑的开发者阅读，文末附Transformer架构解析预告。

>难的从来不是数学，而是对数学本质的理解;
>复杂的从来不是符号，而是对符号生命力的感悟。
 
>这篇内容适合已经掌握深度学习与神经网络基础的小伙伴阅读，特别是对「自注意力机制」设计原理有理解障碍的同学。如果完全没接触过相关概念也不用慌😉 我的下篇文章会从零开始拆解RNN与Transformer架构，记得点个关注，更新第一时间推送~

假设有这样一组输入序列
$$
X=\left[ \begin{array}{c}
	1\\
	2\\
	3\\
\end{array} \right] \left[ \begin{array}{c}
	1\\
	2\\
	5\\
\end{array} \right] \left[ \begin{array}{c}
	0\\
	2\\
	0\\
\end{array} \right] ...
$$
> 数字是我随便设置的（因为反正也用不到），每一个中括号里面都是一个向量(可以用来表示一个单词、一个token等信息)，我们暂且记为$x_1,x_2,x_3,...$

也就是说
$$
X=\left( x_1,x_2,x_3,... \right) 
$$
我们知道自注意力机制通过计算序列中每个元素对其他元素的"注意"程度来衡量整个$X$的信息，然后将结果传递给解码器，而传统RNN只是将全局信息一股脑作为输出，忽视了不同元素之间的关联关系。我们重点讲解"注意力"是如何通过数学表达实现的。

以第一个序列$x_1$为例，如果要判断它与$x_2$的关联程度，直接对它们进行某种运算虽然能得到包含两个元素信息的结果，但无法有效表征关联程度。因此我们需要先为它们分别赋予权重，再进行运算，这样权重的数值就能表征它们的关联强度。具体实现方式如下图所示：

>![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/30e5a15fb89b489186e7ad824e39f2cf.png#pic_center)

由于要看$x_1$序列对$x_2$和$x_3$序列的注意程度，我们将$x_1$先乘一个矩阵$W_q$，得到新的向量$q_1$，然后将$x_2$和$x_3$乘以同一个矩阵$W_k$得到$k_2$和$k_3$。$W_q$和$W_k$就是我们上文说到的权重了，然后将$q_1$和$k_2$、$k_3$分别作内积(点乘)，这样得到的值就可以分别理解成$x_1$序列对$x_2$和$x_3$序列的“关注”程度了。
等等！我想你一定满脸问号，刚开始我也是这样，我们重新审视这个过程：
- 为什么$x_1$乘以矩阵$W_q$，而$x_2$和$x_3$乘以同一个矩阵$W_k$呢？
	>我想你一定记得，我们一直说的是判断$x_1$对其余序列的关注程度吧，那么$x_1$自然要乘以不同的矩阵作为权重，而$x_2$和$x_3$是被关注对象，所以它们要被乘以同一个矩阵$W_k$，即使$x_1$对它们的关注程度不一样也不用担心$W_k$相同会带来问题，因为$W_k$是一个权重矩阵，可以协调参数充分体现不同序列的被关注程度(如果不理解我后续会更近线性代数中一些概念的本质理解)。
- 同样地，$x_2$也会关注$x_1$和$x_3$等序列，和上述操作一模一样，换个下标就行
- 还有一点，序列自己也会关注自己，所以也要对自己乘以一个矩阵$W_k$，将$k_i$和$q_i$作点乘

下面我们基本上可以考虑开始进行第一步数学推导了：
已知：$X=\left( x_1,x_2,x_3,... \right)$，我们上述操作如下：
$$
\left\{ \begin{array}{l}
	q_1=x_1\cdot W_q\\
	q_2=x_2\cdot W_q\\
	...\\
\end{array} \right. 
$$写成矩阵形式就是
$$
Q=X\cdot W_Q
$$$$
\left\{ \begin{array}{l}
	k_1=x_1\cdot W_k\\
	k_2=x_2\cdot W_k\\
	...\\
\end{array} \right. 
$$写成矩阵形式就是
$$
K=X\cdot W_K
$$详细的每个向量作内积我不再演示，直接给出矩阵形式：
$$
Q\cdot K^T
$$
>转置一下是为了符合向量内积表达


现在我们得到的这个矩阵$Q\cdot K^T$，每个位置的数字就代表对应一个序列对另一个序列的关注程度。这里还需要进行缩放操作（主要出于两点考虑：防止内积过大导致后续Softmax运算不稳定，以及常规处理方法（通常将点积结果除以矩阵维度的平方根）。具体原理我会在后续博客专门讨论）。接着对矩阵中的数值进行Softmax归一化处理，这样得到的分布就更符合概率形式的表达。

现在我们掌握了任意两个序列间的相互关注程度，但作为编码器的输出还缺少什么呢？没错，就是序列自身的内容信息！我们采用相同的方法对每个序列施加$W_v$变换，将得到的一系列新向量重新组合（推导逻辑与上文所述基本一致），直接给出矩阵形式：
$$
V=X\cdot W_v
$$
接着我们将$V$乘以$Q\cdot K^T$得到 $Q\cdot K^T\cdot V$,把这个作为编码器输出，解码器输入即可。

到这里相信你对于自注意力机制基本掌握了，如果你有不会的概念，可以期待我的下一篇博客，我会深入讲解RNN和Transformer，带你了解大预言模型的底层原理。

**结束了吗？**

我们再回头看，我们如何站在上帝视角去审视上述的三个权重矩阵$W_q$、$W_k$和$W_v$呢？或者说我们怎么理解$Q$、$K$和$V$这三个矩阵呢？

​这三个矩阵是查询（Query）​、键（Key）​ 和 ​值（Value）三个单词的首字母，想想看，我们上述本质上用$Q$衡量序列对序列的关注程度，用$K$衡量序列的被关注程度，用$V$衡量序列的内容。这不正是查询所需相关信息、展示位置(键)以供查询和提取内容(值)这三个操作吗？

站在历史的角度，当初设计出这种机制的人又何尝不是天才？
