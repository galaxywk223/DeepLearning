# 从备考策略到代码实战：手写数字识别的神经网络全解


> 本文以备考策略为独特视角，用五步学习法拆解机器学习全流程。通过手写数字识别任务，深入剖析数据预处理、全连接网络构建、梯度下降优化的数学原理，结合PyTorch代码逐行实现MNIST识别模型（测试准确率达97.38%）。文中独创性提出备考错题本与反向传播的对应关系，用可视化结果展现神经网络从像素荒漠中解码数字特征的思维轨迹，为初学者打通理论推导与工程实践的认知闭环。

## 从备考策略看机器学习（Machine Learning）：

初学机器学习时，"梯度下降"、"过拟合"等术语常令人望而生畏。有趣的是，这些概念与备考策略存在精妙对应。我们不妨以备考五部曲为引，揭开机器学习的神秘面纱。

**一、资料整理：数据预处理**
如同整理复习资料需剔除残破试卷，数据预处理首先要清洗缺失值与异常数据。统一答题卡格式对应着标准化与编码转换，将不同量纲的特征转化为可比数值。最后需区分真题与模拟题，按6:2:2比例划分训练集、验证集和测试集，正如备考时需明确区分基础训练与模拟考场。

> 破损试卷会误导复习方向，脏数据就像残缺的复习材料，会让模型建立错误认知。

**二、复习规划：模型架构**
制定复习计划要量体裁衣：基础薄弱者主攻教材例题，正如简单问题适用线性回归；冲刺高分者钻研竞赛压轴题，复杂场景则需要神经网络这类高阶模型。选择模型如同选择备考策略，需在能力边界与目标难度间找到平衡点。

**三、错题精炼：梯度下降**
备考中的错题修正机制完美对应反向传播原理。首次遇到难题时的错误预测如同神经网络的随机参数初始化，对照标准答案的过程相当于计算损失函数梯度，而修正解题思路正是参数迭代更新。这个动态调整过程既需要持续纠偏，也要避免过度反应。

> 追求"三天逆袭"就像设置过高学习率，知识体系容易崩塌；机械抄写错题则像学习率过低，表面勤奋却无法突破瓶颈。

**四、模考检验：拟合诊断**
验证集如同模考试卷，能暴露真实的知识掌握程度。当模拟考试全盘皆输，对应模型欠拟合，揭示基础概念存在漏洞；若能完美复现旧题却无法应对新题型，则是过拟合的典型症状，说明陷入死记硬背的误区；稳定保持高分的学生，才像健康模型那样真正掌握解题方法论。

> 真正的学霸不会被题型变化难倒，优秀模型的终极考验在于处理未见数据时的从容姿态。

**五、终极考核：泛化能力**
最终考场（测试集）是试金石：依赖题海战术者可能被新题型击溃，对应模型机械记忆训练数据；善用公式推导者能破解陌生题目，展现模型的基础泛化能力。

当我们理解这些备考策略与机器学习的深层共鸣，就能更自然地迈出实践第一步。现在，让我们开始这场特别的学习之旅。

## PyTorch核心组件解析

作为深度学习领域的当红框架，PyTorch以其动态计算图和易用性俘获了大批开发者的心。

#### 导入必备工具包：


```python
import torch
from torchvision import transforms, datasets
import torch.nn as nn
import torch.optim as optim
```

这些库的用途如下：
- torch: PyTorch 核心库，提供张量计算和自动微分功能
- transforms: 提供图像预处理工具
- datasets: 提供常见数据集
- torch.nn: 神经网络模块
- optim: 优化算法模块

## 数据预处理与特征工程（Feature Engineering）

通过前面的考试案例，我们已经明白机器学习要从数据导入开始。但这里的数据可不是普通的数字，而是承载着手写数字的灰度图片！不过别担心，数学能帮我们破解图像的奥秘——通过解析每个像素的灰度值，整张图片就能转换为数学上的张量（Tensor）结构。

说到张量，可能你会问：这玩意儿和向量有啥区别？我们熟悉的向量就像坐标系里的箭头，而张量其实是它的高维升级版，能够通过多维嵌套来承载更复杂的数据结构。


```python
# 数据预处理
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,)),
    transforms.Lambda(lambda x: x.view(-1))
])

# 加载数据集
train_dataset = datasets.MNIST(
    root='./data', train=True, download=True, transform=transform
)
test_dataset = datasets.MNIST(
    root='./data', train=False, download=True, transform=transform
)
```

初见这段代码可能会有点懵圈，我们来庖丁解牛：

虽然代码里transform写在数据集加载之前，但这不是"早产"操作。这里其实是在搭建数据处理流水线，真正的预处理会和数据加载同步进行——就像给快递分拣机装好了传送带，包裹（数据）经过扫描（处理）后才到你手上。

- `train_dataset`和`test_dataset`，前者含6万训练样本，后者含1万测试样本（暂不涉及第三者验证集）
- 参数说明书：
  - `root`：数据集的地址
  - `train`：身份标识（True=训练集，False=测试集）
  - `download`：下载开关（没数据时自动启动）
  - `transform`：处理流水线

**transform解析**  
1. **ToTensor**  
   把PIL图像变成PyTorch张量，自动归一化（Normalization）到[0,1]区间。

2. **Normalize标准化**  
   用MNIST的全局统计量（μ=0.1307, σ=0.3081）做数据中心化。假设某批训练数据意外集中在0.99高亮区，标准化后依然能映射到标准正态分布。

3. **Lambda展平**  
   将28x28的二维矩阵拍平成784维向量，为全连接网络准备好"标准输入尺寸"

## 神经网络数学原理深度解析

### Hard sigmoid引入非线性
经过前文的探讨，我们终于要揭开神经网络的核心机制。让我们从一个具体的数据案例入手：

假设存在如图所示的序列数据（注：实际数据点远多于图中5个示意点，整体呈现多段近似线性的分布特征）：
> ![在这里插入图片描述](../assets/images/mlp-mnist/de2ca7bc7b1f489f8834f2e151b2fcc8.png)


面对这种多段线性分布的数据，传统的单一线性回归模型显然力不从心。但有趣的是，每个区段本身都保持着良好的线性特征，这个矛盾该如何破解？

我们尝试将不同区段视为独立线性模型：
$$y=b_i+w_i\cdot x$$
其中下标i表示第i个区段。对应的定义域划分可表示为：
$$\begin{cases}y=b_1+w_1x&x_1<x<x_2\\y=b_2+w_2x&x_2<x<x_3\\\vdots&\vdots&\end{cases}$$
但直接套用这个模型会遇到棘手的问题：机器学习需要同时优化各段的斜率$w_i$、截距$b_i$以及区段分界点$x_i$，而分界点之间还存在$x_{i+1}>x_i$的约束关系。这使得参数空间复杂度呈指数级增长。

为此我们引入坐标平移技巧：将每个区段的左端点平移到原点，同时保持定义域外的输出恒定。改造后的函数形式变为：
$$y_i(x)=\begin{cases}0&x<x_i\\b_i+w_i(x-x_i)&x_i\leq x\leq x_{i+1}\\b_i+w_i(x_{i+1}-x_i)&x>x_{i+1}&&\end{cases}$$
> （注：这里的$b_i$与原始模型中的截距存在差异，但模型会自动学习参数间的关联关系）

可视化效果如图所示：
> ![在这里插入图片描述](../assets/images/mlp-mnist/b05cbe5bd0d44591ad6999db3dc8c17b.png)


此时整体预测函数可以表示为这些基函数的线性叠加。但各段表达式仍存在形式不统一的问题，这暗示着我们需要更本质的数学表达来整合这些分段特征。

让我们聚焦在关键参数的优化上。当前各分段函数的偏置项、权重系数以及定义域端点各不相同，这种参数冗余严重影响计算效率。我们尝试通过标准化输出范围来简化模型：既然起始点y=0可通过坐标平移统一，那么终止点y=1同样可以标准化。

通过引入缩放因子$c_i$对分段函数进行压缩处理，得到新表达式：
$$y=c_i\cdot\begin{cases}0&x<x_i\\b_i+\frac{1}{x_{i+1}-x_i}x&x_i<x<x_{i+1}\\1&x>x_{i+1}&&\end{cases}$$
此时需要确定的参数缩减为$b_i$、$c_i$和$x_i$ (注：$x_{i+1}$可通过$x_i$和相邻参数推导获得)。为进一步简化，我们定义关键参数：
$$w_i\equiv\frac1{x_{i+1}-x_i}$$
这本质上是将区间宽度转化为可学习的权重参数。重构后的标准化分段函数呈现统一形式：
$$y=c_i\cdot\begin{cases}0&x<x_i\\b_i+w_ix&x_i<x<x_{i+1}\\1&x>x_{i+1}&&\end{cases}$$
(重要提示：此时的$b_i$和$w_i$已不再是原始线性模型的截距和斜率，而是融合了区间位置信息的复合参数)
这种具有标准输出范围的分段函数被称为Hard Sigmoid函数。整个预测模型最终可表示为：
$$y=\mathrm{const}+\sum\begin{bmatrix}\mathrm{Hard_sigmoid}(x)\end{bmatrix}$$
在实际训练中，模型仅需通过梯度下降优化三组核心参数：缩放因子$c_i$、修正权重$w_i$和位移参数$b_i$。这种参数化方式不仅大幅降低了计算复杂度，更为后续引入连续可导的激活函数（Activation Function）埋下伏笔。
### sigmoid函数（Sigmoid Function）
让我们突破分段函数的桎梏。虽然Hard Sigmoid实现了参数精简，但数学表达依然不够优雅。这时，一个经典函数的改造方案浮出水面——Sigmoid函数：
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
其全域可导的特性与S型曲线（如图）完美契合分段线性趋势：
> ![在这里插入图片描述](../assets/images/mlp-mnist/d730d42c4ad34214843afda68f58cc14.png)


但要让Sigmoid真正替代Hard Sigmoid，需要赋予其三个核心调控能力：
1. **值域缩放**：通过前置系数$c_i$控制输出幅度
2. **水平平移**：在输入中引入偏置$b_i$实现位置偏移
3. **斜率调节**：通过权重$w_i$控制曲线陡峭程度

改造后的增强型Sigmoid呈现为：
$$
y = c_i \cdot \sigma(w_ix + b_i) 
$$
其中：
- $w_i$调节曲线上升速率（等效改变定义域跨度）
- $b_i$控制曲线水平位置
- $c_i$缩放输出范围

此时整体预测模型可简化为：
$$
y = b + \sum_i c_i \cdot \sigma(b_i + w_ix)
$$
当输入变量扩展为多维时（如$x_1,x_2,...x_n$），每个神经元可视为：
$$
\sigma(b_i + \sum_j w_{ij}x_j)
$$
这自然导出标准神经网络表达式：
$$
y = b + \sum_i c_i \cdot \sigma\left( b_i + \sum_j w_{ij}x_j \right)
$$
（注：常量$b$已吸收原式中的const项，作为整体偏置项存在）

这种参数化改造实现了三大突破：
- 通过连续可导函数替代分段不连续函数
- 将离散的参数优化转化为连续空间搜索
- 保留原模型参数的可解释性（$c_i$对应区段权重，$w_{ij}$对应特征影响强度）

至此，我们完成了从分段线性模型到神经网络架构的自然过渡。这个推导过程揭示了神经网络本质上是多个可调Sigmoid基函数的线性组合，每个神经元对应一个特征空间中的非线性划分单元。

让我们通过结构图解析神经网络的运算本质。参考流程示意图：
> ![在这里插入图片描述](../assets/images/mlp-mnist/90dc1fbe58d4462eb8c7fc0c75c21d91.png)


**计算流程解析**：
1. **特征融合层**：输入特征向量$x=[x_1,x_2,x_3]^T$与权重矩阵$W$进行线性组合
   $$
   r = b + Wx \quad \text{其中} \quad W=\begin{bmatrix}
   w_{11} & w_{21} & w_{31}\\
   w_{12} & w_{22} & w_{32}\\
   w_{13} & w_{23} & w_{33}
   \end{bmatrix}
   $$
   该运算实现了三维特征空间到隐层空间的投影

2. **非线性激活**：通过sigmoid函数逐元素变换
   $$
   a = \sigma(r) = [\sigma(r_1), \sigma(r_2), \sigma(r_3)]^T
   $$
   此时每个神经元输出被压缩到(0,1)区间

3. **决策输出层**：对激活信号进行加权合成
   $$
   y = b_{out} + c^Ta
   $$
   其中$c=[c_1,c_2,c_3]^T$为输出层权重向量

**模型退化实验**：若移除sigmoid函数，整个系统将坍缩为
$$
y = b_{out} + c^T(b + Wx) = (b_{out}+c^Tb) + c^TWx
$$
这正是线性回归模型，验证了非线性激活函数的必要性。


### 生物神经元机制
让我们揭开神经网络与生物神经系统之间的深刻联系。当特征向量通过线性组合生成中间量$r$后，关键的生物模拟机制开始显现——sigmoid激活函数精确复刻了神经元的信号整合原理：

神经突触需要积累足够浓度的神经递质才会触发动作电位，sigmoid函数在这里承担着类似阈值的判断功能。当组合信号输入sigmoid函数时，低于阈值的信号会被压缩趋近于0（类似神经元的静息状态），而高于阈值的信号则被放大趋近于1（神经冲动传导）。

我们可以在上述流程中再增加一步以便更好的理解这一过程：
> ![在这里插入图片描述](../assets/images/mlp-mnist/2ffba6ed39f74e019eb50dea9fb0a467.png)


- 在流程图中新增的激活环节揭示了一个关键现象——首层sigmoid函数可能将部分信号归零，只有有效信号能传递到下一层继续计算。这种层层筛选的结构恰似生物神经网络的工作模式，"神经网络"（Neural Network）的命名正源于此。
- 观察模型结构时，第一张示意图仅含单层变换，第二张则出现双层结构。实际应用中这种分层可达数十层，每增加一层就意味着模型能捕捉更复杂的特征交互（如从识别线条到组合成五官），这种堆叠层级的技术被称为"深度学习"（Deep Learning）。

## 模型架构（Model Architecture）设计与实现
让我们回到手写数字识别的核心问题。经过前期处理，图像特征已被展平为784维的一维张量（对应28×28像素）。接下来需要构建的，是一个能将这784维空间映射到0-9数字分类的神经网络模型。

> 在模型设计前需要明确输出形式：虽然最终需要具体数字（如"7"），但机器学习中通常采用概率分布表示——通过10维向量（对应0-9）实现one-hot编码。这种方式既保留分类不确定性，又便于计算交叉熵损失。例如数字7的理想输出应为：
$$
\left( \begin{matrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0
\end{matrix} \right) ^{\tau}
$$

模型延续前文思路但进行扩展：
- **输入层**：784节点（对应图像像素）
- **隐含层**：若干节点（可调节的超参数）
- **输出层**：10节点（输出概率分布）

具体结构如下图所示：
> ![在这里插入图片描述](../assets/images/mlp-mnist/ab3150a8e7bb448ba31ecb1b652b8b49.png)


### ReLU的数学优势
可以看出，激活函数的选择发生了关键转变：从S形曲线（sigmoid）切换为折线型ReLU（Rectified Linear Unit）。这种改变看似简单却蕴含着精妙的数学设计：
$$
\text{ReLU}(x) = \max(0,x)
$$
尽管形式比sigmoid简洁，但二者在前驱计算阶段的处理方式存在共性：

| 特性        | Sigmoid                      | ReLU                    |
|-------------|------------------------------|-------------------------|
| 前驱计算    | 保留$w \cdot x + b$线性结构  | 同样保留线性计算基础     |
| 输出范围    | 压缩至[0,1]区间              | 非负无界输出            |
| 计算效率    | 需计算指数函数               | 只需阈值判断            |

ReLU的无界特性可能看似缺陷，但正是这种稀疏激活特性使其成为Transformer等现代深度架构的基石。

（注：ReLU在深层网络中的梯度传播优势，将在后续文章展开说明）


现在让我们用PyTorch实现这个多层感知机。核心代码如下：


```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # 输入到隐层的线性投影
        self.act = nn.ReLU()             # 非线性激活函数
        self.fc2 = nn.Linear(128, 10)    # 隐层到输出的映射

    def forward(self, x):
        x = self.fc1(x)  # 展开的784维向量→128维隐空间
        x = self.act(x)  # 引入非线性决策边界
        return self.fc2(x)  # 输出logits空间

model = MLP()
```

代码解析：
1. **架构构建**：
   - `nn.Linear(28*28, 128)` 将展平的784维像素映射到128维隐层特征空间
   - `ReLU()` 作为非线性转换器，突破线性模型的局限性
   - `nn.Linear(128, 10)` 最终将特征压缩到10维logits空间（对应数字类别）

2. **前向传播**：
   输入张量依次经历：
   ```
   像素空间 → 线性变换 → ReLU激活 → 分类空间
   ```

3. **模型实例化**：
   `MLP()` 调用时自动初始化权重矩阵和偏置向量，构成可训练参数集


这里存在一个关键问题：虽然输出层设计为10维向量，但未经处理的原始输出（logits）并不能直接解释为概率。让我们深入分析这个现象：

**核心矛盾**：
- **标签形式**：训练时使用one-hot编码（如`[0,0,...,1,...,0]`）
- **模型输出**：实际产生的是未归一化的logits（如`[-5.1, ...,9.5,...]`）

**典型输出示例**：
| 阶段         | 输出向量示例（转置表示）                     | 特性分析                     |
|--------------|------------------------------------------|----------------------------|
| 初始化阶段   | $[-0.3,1.2,0.5,...,1.0]^\tau$           | 各维度值在(-∞,+∞)随机分布     |
| 训练收敛后   | $[-5.1,-3.2,...,9.5,...,-4.3]^\tau$     | 目标类logits显著高于其他维度  |


为什么输出层不直接使用softmax生成概率？将在讨论损失函数时详细展开。


```python
# 设置设备
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)
```

这一步没什么好说的，如果可以用GPU加速运算就用GPU，否则就用CPU

## 损失函数（Loss Function）与优化算法（Optimization Algorithm）
### 交叉熵损失函数
现在进入模型训练的核心环节。就像学生需要通过考试成绩检验学习成果，机器学习模型也需要用量化指标来评估预测效果，这个核心指标就是损失函数（Loss Function）。

我们选用交叉熵损失函数（Cross-Entropy Loss），其背后有坚实的信息论支撑：它能精确衡量真实概率分布与预测分布之间的信息差异，尤其适合分类场景。在数字识别任务中，真实标签P是确定的one-hot编码（比如数字7对应[0,0,0,0,0,0,0,1,0,0]），此时交叉熵公式简化为：
$$
H(P,Q) = -\log Q(c)
$$
其中c代表真实类别（如数字7），Q(c)是模型对该类别的预测概率。

这个公式的数学特性完美契合分类需求：
- 当预测概率Q(c)趋近1时（完全正确），损失值趋近0；当Q(c)趋近0时（完全错误），-log(Q(c))会急剧增大，形成指数级惩罚
- 可能有读者会问：为何不用1-Q(c)这类线性函数？试想当预测概率为0.5时，线性函数仅给出0.5的损失，而交叉熵的-log(0.5)=0.693能更强烈地警示"模棱两可"的预测，这正是对数函数的独特优势——它在0-1区间具有指数级的梯度变化

这种非对称的惩罚机制，既奖励高置信度的正确预测，也严厉惩戒低置信度的错误预测，驱动模型快速收敛到理想状态。

### 梯度下降（Gradient Descent）
现在进入模型优化的关键环节。寻找最优模型参数的过程，本质上是求解损失函数的极值问题。在机器学习领域，梯度下降法（Gradient Descent）是这类优化问题的经典解法。但传统全量梯度下降存在明显局限——每次迭代需计算全量数据集的误差梯度，这在面对百万级数据时将产生难以承受的计算开销。

为此，工程实践中常采用三种优化范式：
1. **随机梯度下降（SGD）**：每次仅用单个样本更新参数，效率极高但路径震荡剧烈
2. **小批量梯度下降**：将数据集划分为若干批次（batch），每次用整批样本计算平均梯度，兼具效率与稳定性
3. **动量梯度下降**：通过引入历史梯度信息，有效抑制震荡加速收敛（进阶技巧，暂不展开）

让我们通过图片直观理解梯度下降的数学本质。假设参数空间中的损失函数如下图所示：
> ![在这里插入图片描述](../assets/images/mlp-mnist/925e7a260e7f4916b6da747d4cadbd7e.png)


初始参数通常随机设置（如图中起点），此时梯度下降算法将执行以下关键操作：
1. **梯度计算**：求取当前位置的损失函数梯度（曲面切线方向的斜率）
2. **参数更新**：沿梯度反方向移动参数（数学上体现为负梯度方向）
3. **迭代终止**：当梯度模长小于阈值或达到最大迭代次数时停止

梯度方向的物理意义在图中尤为直观：
> ![在这里插入图片描述](../assets/images/mlp-mnist/df4db9b06da7434281306d533d0cfee4.png)

- **负斜率区域**（左图）：向x轴正方向移动可降低损失值
- **正斜率区域**（右图）：向x轴负方向移动才有效

其中学习率η的设定堪称艺术：
- **梯度绝对值大**（陡坡）：采用大步伐快速下坡
- **梯度绝对值小**（缓坡）：切换小碎步精细调节

这自然引出梯度下降的标准更新公式：
$$\theta_{new} = \theta_{old} - \eta \cdot \nabla J(\theta_{old})$$
式中∇J(θ)为损失函数的梯度向量，η为学习率（Learning Rate）（需手动设定的超参数）。当参数空间扩展到N维时，该公式依然保持形式不变，此时梯度向量即为各个参数方向的偏导数集合。

这种基于局部梯度信息的优化策略，虽然不能保证找到全局最优解，但在深度学习的非凸优化场景中展现出惊人的实用性，成为现代机器学习模型的基石算法。

现在进入代码实战环节。在PyTorch框架中，定义损失函数与优化器仅需两行简洁的代码：


```python
# 配置损失函数与优化器
criterion = nn.CrossEntropyLoss()  # 内置自动softmax的交叉熵损失
optimizer = optim.SGD(model.parameters(), lr=0.01)  # 小批量梯度下降优化器
```

**代码解析：**

1. **智能化的损失函数**  
   `CrossEntropyLoss()`的设计暗藏玄机——它会在计算交叉熵前自动对模型输出执行Softmax操作。这与我们之前讨论的理论完美衔接：
   这种"开箱即用"的特性避免了重复编码，同时确保数值稳定性。

2. **优化器的参数配置**  
   - `model.parameters()`：动态获取神经网络所有可训练参数（权重矩阵、偏置向量等）
   - `lr=0.01`：设置学习率为0.01，控制参数更新步长（可类比为下山时的步幅）

**数学原理：**  
Softmax函数的数学本质是将原始输出转换为概率分布：
$$
\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \quad \text{其中} \begin{cases} 
\sigma(\mathbf{z})_j \in (0,1) \\
\sum_{j=1}^K \sigma(\mathbf{z})_j = 1 
\end{cases}
$$
PyTorch的`CrossEntropyLoss`实际上等价于`nn.LogSoftmax() + nn.NLLLoss()`的组合，这种实现方式在数值计算上更为稳定。

## 训练流程与性能评估

现在进入模型训练的核心实现环节。让我们用PyTorch框架构建高效的数据管道和训练流程：

### 数据加载器配置


```python
# 构建数据管道
batch_size = 32  # 硬件友好的批处理量
train_loader = torch.utils.data.DataLoader(
    train_dataset, 
    batch_size=batch_size,
    shuffle=True  # 打乱样本顺序避免记忆效应
)
test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False  # 测试集保持确定顺序
)
```

**关键配置解析：**
- `shuffle=True` 通过随机化样本顺序打破数据相关性，增强模型泛化能力（Generalization Ability）
- 批量大小32是经过实践验证的平衡值，在内存效率与梯度稳定性间取得折中


### 训练循环详解


```python
# 模型训练与评估
num_epochs = 10  # 完整遍历训练集10次
for epoch in range(num_epochs):
    # 训练阶段
    model.train()  # 启用训练模式（Dropout/BatchNorm生效）
    for images, labels in train_loader:
        # 数据迁移到计算设备（GPU/CPU）
        images, labels = images.to(device), labels.to(device)
        
        # 核心训练步骤
        optimizer.zero_grad()       # 清除历史梯度
        outputs = model(images)     # 前向传播
        loss = criterion(outputs, labels)  # 损失计算
        loss.backward()             # 反向传播求梯度
        optimizer.step()            # 参数更新

    # 评估阶段
    model.eval()  # 切换评估模式（关闭Dropout）
    correct, total = 0, 0
    with torch.no_grad():  # 关闭梯度计算节省内存
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)  # 获取预测类别
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    # 输出性能指标
    print(f"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {100 * correct / total:.2f}%")

```

    Epoch [1/10], Test Accuracy: 92.59%
    Epoch [2/10], Test Accuracy: 94.29%
    Epoch [3/10], Test Accuracy: 95.44%
    Epoch [4/10], Test Accuracy: 95.98%
    Epoch [5/10], Test Accuracy: 96.49%
    Epoch [6/10], Test Accuracy: 96.81%
    Epoch [7/10], Test Accuracy: 97.01%
    Epoch [8/10], Test Accuracy: 97.31%
    Epoch [9/10], Test Accuracy: 97.35%
    Epoch [10/10], Test Accuracy: 97.38%
    

**代码逻辑分解：**

1. **训练模式控制**  
   - `model.train()` 激活所有可训练层（如Dropout、BatchNorm）
   - `model.eval()` 冻结特殊层，确保评估结果的一致性

2. **设备迁移**  
   `to(device)` 将数据自动调度到GPU/CPU，充分利用硬件加速

3. **梯度管理**  
   - `zero_grad()` 防止梯度累积导致参数更新偏差
   - `no_grad()` 上下文管理器节省约30%的显存占用

4. **性能评估**  
   `torch.max(outputs, 1)` 提取预测类别索引，与真实标签对比计算准确率

**技术细节提示：**
- 每个epoch后重新打乱训练数据，确保模型不会记忆样本顺序
- 测试集保持原始顺序，便于结果复现和错误分析
- 准确率逐步提升的趋势（92.48% → 97.29%）验证了模型的有效收敛

## 可视化

我们人生中第一个深度学习领域的"Hello World"就这么完成了，如果你还想要更直观的结果，可以有编写可视化代码：


```python
import matplotlib.pyplot as plt  # 新增导入
# 新增可视化部分
model.eval()
dataiter = iter(test_loader)
images, labels = next(dataiter)
images = images.to(device)

# 预测
outputs = model(images)
_, preds = torch.max(outputs, 1)

# 将数据转移到CPU并反归一化
images = images.cpu().numpy()
labels = labels.cpu().numpy()
preds = preds.cpu().numpy()

# 反归一化处理并调整形状
images = images * 0.3081 + 0.1307  # 恢复原始像素范围
images = images.reshape(-1, 28, 28)  # 调整形状为28x28

# 可视化前5张图片
num_images = 5
fig, axes = plt.subplots(1, num_images, figsize=(12, 3))
for i in range(num_images):
    ax = axes[i]
    ax.imshow(images[i], cmap='gray')
    ax.set_title(f'True: {labels[i]}\nPred: {preds[i]}', fontsize=10)
    ax.axis('off')
plt.tight_layout()
plt.show()
```


    
![在这里插入图片描述](../assets/images/mlp-mnist/24eca9e6098240f28db8e2a8c8303d78.png)

    


当我第一次跑出这个结果的时候，内心还是有抑制不住的激动，不知各位是何感受

## 后记
作为刚入门深度学习的新手，这次搭建的神经网络仅包含单个隐藏层，可视为最基础的前馈网络结构。在后续学习中，我计划逐步探索更复杂的网络架构：尝试增加网络深度、引入更先进的优化算法，最终向卷积神经网络等复杂模型进阶，期待与各位共同成长。

此外，本文的神经网络数学原理探究部分思想参考自台湾大学李宏毅教授在YouTube的视频，部分名词理解参考自复旦大学邱锡鹏教授的《神经网络与深度学习》一书。

Email: [wangk2829@gmail.com](mailto:wangk2829@gmail.com)
