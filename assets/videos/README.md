# 演示视频说明

视频文件：`mnist-demo.mp4`

这段视频展示了一个较完整且逻辑清晰的 PyTorch 深度学习项目实战演示：从 MNIST 手写数字识别出发，通过特征图可视化解释 CNN 内部机制，最后进阶到更复杂的图像分类任务并展示工程化优化思路。

> 说明：视频后半段关于 CIFAR-10 的工程化优化代码因重装系统遗失，目前仓库仅保留视频与 MNIST 复现实验代码。

## 内容摘要（按时间轴）

### 1) 基础篇：MNIST 手写数字识别（00:00 - 01:26）

- 数据处理：`torchvision` 加载 MNIST；随机旋转、转 Tensor、标准化
- 工程细节：`pin_memory`、`num_workers`、`batch_size` 等加速与效率配置
- 模型搭建：CNN（卷积/BN/ReLU/池化）+ 全连接层 + Dropout
- 训练与验证：标准训练循环（`zero_grad` → `backward` → `step`）
- 结果：测试集准确率约 99%，对一定旋转扰动有鲁棒性

### 2) 原理解析：特征图可视化（01:27 - 02:01）

- Conv1：边缘/角点等基础特征
- ReLU：抑制负响应，保留正激活
- MaxPool：降维并突出关键信号
- Conv2：组合更高级的模式（弧线、交叉点等）

### 3) 进阶篇：CIFAR-10 挑战与优化（02:02 - 03:20）

- 现象：简单 CNN 在彩色图像任务上表现受限（约 72%）
- 工程化迭代思路：数据增强 / 架构加深 / 训练策略 / 正则化
  - 数据增强：随机裁剪、翻转、颜色抖动等
  - 架构：更深的卷积网络 + 残差连接
  - 训练：AdamW、动态学习率、混合精度（AMP）
  - 正则化：标签平滑、早停等
- 效果：准确率提升至 90%+（训练轮次减少时仍可达 88% 左右）

### 4) 总结（03:21 - 结束）

- 核心理念：基准模型 → 从数据/架构/训练三条线迭代优化 → 持续改进
